{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Example Implementation with Vertex AI"
      ],
      "metadata": {
        "id": "Ue-3XHDTYxYY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "eajiIXGWBjAN",
        "outputId": "e3bf69f9-9e44-4908-9202-e42b0042fa74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.1)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.8.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.9.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.2)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.6.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "#Install Required Libraries\n",
        "!pip install google-cloud-aiplatform transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.cloud.aiplatform as aiplatform\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "\n",
        "# Set up your project and location (ensure that your environment is authenticated)\n",
        "PROJECT_ID = 'lawgpt-423703'\n",
        "REGION = 'us-central1'  # or another region you are using\n",
        "\n",
        "# Initialize the Vertex AI client\n",
        "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer from Hugging Face\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Tokenize input text\n",
        "input_text = \"Vertex AI makes deploying models easy.\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "# Run inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# Get prediction\n",
        "logits = outputs.logits\n",
        "predicted_class = torch.argmax(logits).item()\n",
        "\n",
        "print(f\"Predicted class: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4msfeEGAgSF7",
        "outputId": "7e773140-5859-4610-b3ce-116c72d1fd4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Storing and Querying Data with Google Cloud Storage and BigQuery\n"
      ],
      "metadata": {
        "id": "WzRLaTs2hX_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set up GCS and BigQuery authentication\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Install Google Cloud libraries if needed\n",
        "!pip install --upgrade google-cloud-storage google-cloud-bigquery\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "collapsed": true,
        "id": "R8Z-ql0ygdZb",
        "outputId": "e3b34a7f-e256-4752-bb83-88d054528345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Collecting google-cloud-storage\n",
            "  Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.10/dist-packages (3.25.0)\n",
            "Collecting google-cloud-bigquery\n",
            "  Downloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.27.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0dev,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (24.1)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.65.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (3.20.3)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage) (1.24.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]<3.0.0dev,>=2.11.1->google-cloud-bigquery) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage) (4.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage) (0.6.1)\n",
            "Downloading google_cloud_storage-2.18.2-py2.py3-none-any.whl (130 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.5/130.5 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_bigquery-3.26.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: google-cloud-storage, google-cloud-bigquery\n",
            "  Attempting uninstall: google-cloud-storage\n",
            "    Found existing installation: google-cloud-storage 2.8.0\n",
            "    Uninstalling google-cloud-storage-2.8.0:\n",
            "      Successfully uninstalled google-cloud-storage-2.8.0\n",
            "  Attempting uninstall: google-cloud-bigquery\n",
            "    Found existing installation: google-cloud-bigquery 3.25.0\n",
            "    Uninstalling google-cloud-bigquery-3.25.0:\n",
            "      Successfully uninstalled google-cloud-bigquery-3.25.0\n",
            "Successfully installed google-cloud-bigquery-3.26.0 google-cloud-storage-2.18.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "b41a6ad7aca94f47b1cb490ec2ecf663"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into GCS and query using bigquery\n",
        "from google.cloud import bigquery\n",
        "\n",
        "PROJECT_ID = 'lawgpt-423703'\n",
        "GCS_BUCKET = 'my-legal-data-bucket'\n",
        "\n",
        "# Initialize the BigQuery client\n",
        "bigquery_client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "# Define the dataset and table IDs\n",
        "dataset_id = 'sample_dataset'  # Make sure to change to your preferred dataset name\n",
        "table_id = 'sample_data'  # This is your table name\n",
        "\n",
        "# Create the dataset if it doesn't exist\n",
        "dataset_ref = bigquery_client.dataset(dataset_id)\n",
        "\n",
        "try:\n",
        "    # Try to get the dataset, if it exists\n",
        "    dataset = bigquery_client.get_dataset(dataset_ref)\n",
        "    print(f\"Dataset {dataset_id} already exists.\")\n",
        "except:\n",
        "    # Create the dataset if it doesn't exist\n",
        "    dataset = bigquery.Dataset(dataset_ref)\n",
        "    dataset.location = \"US\"  # Set location, you can change this\n",
        "    dataset = bigquery_client.create_dataset(dataset)\n",
        "    print(f\"Created dataset {dataset_id}.\")\n",
        "\n",
        "# Create a table schema for your data\n",
        "schema = [\n",
        "    bigquery.SchemaField(\"id\", \"INTEGER\"),\n",
        "    bigquery.SchemaField(\"name\", \"STRING\"),\n",
        "    bigquery.SchemaField(\"age\", \"INTEGER\"),\n",
        "]\n",
        "\n",
        "# Set the table reference\n",
        "table_ref = bigquery_client.dataset(dataset_id).table(table_id)\n",
        "\n",
        "# Configure the load job\n",
        "job_config = bigquery.LoadJobConfig(\n",
        "    schema=schema,\n",
        "    source_format=bigquery.SourceFormat.CSV,\n",
        "    skip_leading_rows=1,  # Skip header row in CSV\n",
        "    autodetect=False\n",
        ")\n",
        "\n",
        "# Set the GCS URI for your CSV file\n",
        "gcs_uri = f'gs://{GCS_BUCKET}/sample_data.csv'\n",
        "\n",
        "# Load the CSV file from GCS into BigQuery\n",
        "load_job = bigquery_client.load_table_from_uri(gcs_uri, table_ref, job_config=job_config)\n",
        "\n",
        "# Wait for the load job to complete\n",
        "load_job.result()\n",
        "\n",
        "print(f\"Loaded {load_job.output_rows} rows into {dataset_id}.{table_id}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7p91Nwsh4_P",
        "outputId": "5531d15d-1cbc-4976-e7d2-c7e05d9c9f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset sample_dataset already exists.\n",
            "Loaded 3 rows into sample_dataset.sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example Setting Up a Scheduled Backup with Cloud SQL"
      ],
      "metadata": {
        "id": "m5mdA6jgoDJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.auth\n",
        "from google.cloud import sql_v1\n",
        "# Authenticate and create a client\n",
        "credentials, project = google.auth.default()\n",
        "client = sql_v1.SqlBackupRunsServiceClient(credentials=credentials)\n",
        "PROJECT_ID = 'YOUR_PROJECT_ID'\n",
        "INSTANCE_ID = 'MY_INSTANCE_ID'\n",
        "# Define the backup configuration\n",
        "backup_config = {\n",
        "    \"settings\": {\n",
        "        \"backupConfiguration\": {\n",
        "            \"binaryLogEnabled\": True,\n",
        "            \"enabled\": True,\n",
        "            \"startTime\": \"03:00\"\n",
        "        }\n",
        "    }\n",
        "}\n",
        "# Apply the backup configuration to the instance\n",
        "instance_name = \"projects/PROJECT_ID/instances/INSTANCE_ID\"\n",
        "client.update_backup_run(instance=instance_name, backup_run=backup_config)\n"
      ],
      "metadata": {
        "id": "xN3qTkJjnC0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: Enabling Encryption with Customer-Managed Keys"
      ],
      "metadata": {
        "id": "EnQYgyDdjwZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import storage\n",
        "# Initialize the client\n",
        "client = storage.Client()\n",
        "# Define the bucket and key\n",
        "bucket_name = 'my-secure-bucket'\n",
        "kms_key_name= 'projects/my-project/locations/global/keyRings/my-keyring/cryptoKeys/my-key'\n",
        "\n",
        "# Create the bucket with encryption enabled\n",
        "bucket = client.bucket(bucket_name)\n",
        "bucket.encryption = {'defaultKmsKeyName': kms_key_name}\n",
        "bucket.create()\n",
        "print(f\"Bucket {bucket.name} created with encryption.\")"
      ],
      "metadata": {
        "id": "YwhwhQIZjxVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example Setting Up Pub/Sub for Real-time Data Synchronization"
      ],
      "metadata": {
        "id": "zRn5FxNkmpYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import pubsub_v1\n",
        "\n",
        "PROJECT_ID = 'lawgpt-423703'\n",
        "\n",
        "# Initialize clients and names\n",
        "publisher = pubsub_v1.PublisherClient()\n",
        "subscriber = pubsub_v1.SubscriberClient()\n",
        "topic_name = f'projects/{PROJECT_ID}/topics/learning-updates'\n",
        "subscription_name = f'projects/{PROJECT_ID}/subscriptions/learning-sub'\n",
        "\n",
        "# Create topic and subscription\n",
        "publisher.create_topic(name=topic_name)\n",
        "subscriber.create_subscription(name=subscription_name, topic=topic_name)\n",
        "\n",
        "# Publish message\n",
        "publisher.publish(topic_name, b'New learning data available')\n",
        "\n",
        "# Subscribe to messages\n",
        "def callback(msg):\n",
        "    print(f\"Received: {msg.data}\")\n",
        "    msg.ack()\n",
        "\n",
        "subscriber.subscribe(subscription_name, callback=callback)\n",
        "print(\"Listening for messages...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMdS1pobm1mn",
        "outputId": "276fc5a7-a3e4-448b-ef8a-d47ea48a4b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listening for messages...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: Deploying a Model with TensorFlow Lite"
      ],
      "metadata": {
        "id": "iVaCegxxlS8V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Convert a TensorFlow model to TensorFlow Lite\n",
        "model = tf.keras.models.load_model('learning_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "# Save the converted model\n",
        "with open('learning_model.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "id": "diOZIuj1lX_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: Applying Quantization with TensorFlow Lite"
      ],
      "metadata": {
        "id": "9bwxZWjElfDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Convert a TensorFlow model to a quantized TensorFlow Lite model\n",
        "model = tf.keras.models.load_model('learning_model.h5')\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_tflite_model = converter.convert()\n",
        "# Save the quantized model\n",
        "with open('learning_model_quantized.tflite', 'wb') as f:\n",
        "    f.write(quantized_tflite_model)"
      ],
      "metadata": {
        "id": "be91mcaHllIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: Automating Model Retraining with Vertex AI Pipelines"
      ],
      "metadata": {
        "id": "udr-NtN3lue3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kfp.v2 import dsl\n",
        "from google.cloud import aiplatform\n",
        "@dsl.pipeline(name=\"retrain-learning-model-pipeline\")\n",
        "def pipeline():\n",
        "    # Define the retraining and deployment steps\n",
        "    training_job = aiplatform.CustomJob(\n",
        "        display_name=\"model-training-job\",\n",
        "        script_path=\"train_model.py\",\n",
        "        container_uri=\"gcr.io/my-project/training-container\",\n",
        "        model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-1:latest\"\n",
        "    )\n",
        "    training_job.run()\n",
        "# Compile and run the pipeline\n",
        "pipeline.compile(pipeline_func=pipeline, package_path='retrain_pipeline.json')\n",
        "aiplatform.PipelineJob(display_name=\"retrain-pipeline-job\", template_path='retrain_pipeline.json').run()"
      ],
      "metadata": {
        "id": "t3eWnXuGlvYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example: Predicting Learner Performance with Vertex AI"
      ],
      "metadata": {
        "id": "GPcjuJF5l5yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import aiplatform\n",
        "# Initialize the AI Platform client\n",
        "aiplatform.init(project='my-project', location='us-central1')\n",
        "# Define and train a custom model\n",
        "model = aiplatform.CustomJob.from_local_script(\n",
        "    display_name='learner-performance-prediction',\n",
        "    script_path='train_model.py',\n",
        "    container_uri='gcr.io/my-project/training-container',\n",
        "   model_serving_container_image_uri='us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-1:latest'\n",
        ")\n",
        "model.run()"
      ],
      "metadata": {
        "id": "kMlTWo6sl-xX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}