{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1 General Steps"
      ],
      "metadata": {
        "id": "AqAqOD2DEjEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1: Install Google Cloud AI and Storage Libraries\n",
        "!pip install google-cloud-aiplatform google-cloud-storage google-cloud-datalabeling"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "fFNbnEjMFRpx",
        "outputId": "244fbefc-71d3-4a89-f2dd-96592b8f0c1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-aiplatform in /usr/local/lib/python3.10/dist-packages (1.68.0)\n",
            "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.10/dist-packages (2.8.0)\n",
            "Requirement already satisfied: google-cloud-datalabeling in /usr/local/lib/python3.10/dist-packages (1.10.5)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.19.2)\n",
            "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.27.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.20.3)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (24.1)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (3.25.0)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (1.12.5)\n",
            "Requirement already satisfied: shapely<3.0.0dev in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.0.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (2.9.2)\n",
            "Requirement already satisfied: docstring-parser<1 in /usr/local/lib/python3.10/dist-packages (from google-cloud-aiplatform) (0.16)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.7.2)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage) (2.32.3)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.65.0)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.64.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
            "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /usr/local/lib/python3.10/dist-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.1)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->google-cloud-aiplatform) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.8.30)\n",
            "Requirement already satisfied: numpy<3,>=1.14 in /usr/local/lib/python3.10/dist-packages (from shapely<3.0.0dev->google-cloud-aiplatform) (1.26.4)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2: Authenticate with Google Cloud\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Now you are authenticated and ready to interact with your Google Cloud project\n"
      ],
      "metadata": {
        "id": "4WS5l1FeFcv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3: Import the Necessary Libraries and Initialize AI Platform\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')"
      ],
      "metadata": {
        "id": "sVsuk4obFq6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4: Create a new Bucket\n",
        "from google.cloud import storage\n",
        "\n",
        "# Specify your Google Cloud project ID\n",
        "project_id = 'book-examples-2024'  # Your actual project ID\n",
        "\n",
        "# Initialize the Google Cloud Storage client with your project\n",
        "client = storage.Client(project=project_id)\n",
        "\n",
        "# Define a globally unique bucket name\n",
        "bucket_name = 'my-image-data-bucket-unique'  # Change this to something unique\n",
        "\n",
        "# Create the new bucket with a specified location\n",
        "try:\n",
        "    bucket = client.create_bucket(bucket_name, location='us-central1')  # Specify the location\n",
        "    print(f'Bucket {bucket.name} created successfully.')\n",
        "except Exception as e:\n",
        "    print(f'Error creating bucket: {e}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K4Ceqw9G90R",
        "outputId": "dc27bc7e-5af3-4217-93fa-3699b49cddab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bucket my-image-data-bucket-unique created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 7.2.2 Execution\n",
        "#1. Collecting Data with Google Cloud Data Labeling Service"
      ],
      "metadata": {
        "id": "54B3KIrVEraa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJvV_jqODG5h",
        "outputId": "593912f1-f374-4328-8b2e-be263c40e1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.datasets.dataset:Creating ImageDataset\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:Create ImageDataset backing LRO: projects/518684982184/locations/us-central1/datasets/7436657395381567488/operations/6891515304082407424\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:ImageDataset created. Resource name: projects/518684982184/locations/us-central1/datasets/7436657395381567488\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:To use this ImageDataset in another session:\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:ds = aiplatform.ImageDataset('projects/518684982184/locations/us-central1/datasets/7436657395381567488')\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:Importing ImageDataset data: projects/518684982184/locations/us-central1/datasets/7436657395381567488\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:Import ImageDataset data backing LRO: projects/518684982184/locations/us-central1/datasets/7436657395381567488/operations/3674256320279609344\n",
            "INFO:google.cloud.aiplatform.datasets.dataset:ImageDataset data imported. Resource name: projects/518684982184/locations/us-central1/datasets/7436657395381567488\n"
          ]
        }
      ],
      "source": [
        "#1: Create a Dataset for Image Classification\n",
        "# Create an ImageDataset for image classification\n",
        "dataset = aiplatform.ImageDataset.create(\n",
        "    display_name='HITL-dataset',\n",
        "    gcs_source='gs://my-image-data-bucket-unique/images',  # Update with the correct path to your images\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 Create a Labeling Job\n",
        "from google.cloud import datalabeling_v1beta1 as datalabeling\n",
        "\n",
        "# Initialize the Data Labeling API client\n",
        "client = datalabeling.DataLabelingServiceClient()\n",
        "\n",
        "# Define the project path and GCS path for your images and instructions\n",
        "project_id = 'book-examples-2024'\n",
        "bucket_name = 'my-image-data-bucket-unique'\n",
        "\n",
        "# Full project path\n",
        "project_path = f'projects/book-examples-2024'\n",
        "\n",
        "# Step 1: Create an Annotation Spec Set (cats and dogs labels)\n",
        "annotation_spec_set = datalabeling.types.AnnotationSpecSet(\n",
        "    display_name='Animal Classification',\n",
        "    description='Classify images as either cats or dogs',\n",
        "    annotation_specs=[\n",
        "        datalabeling.types.AnnotationSpec(display_name='cats'),\n",
        "        datalabeling.types.AnnotationSpec(display_name='dogs'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Step 2: Create the Annotation Spec Set in Data Labeling Service\n",
        "annotation_spec_set_request = datalabeling.CreateAnnotationSpecSetRequest(\n",
        "    parent=project_path,\n",
        "    annotation_spec_set=annotation_spec_set\n",
        ")\n",
        "\n",
        "created_annotation_spec_set = client.create_annotation_spec_set(request=annotation_spec_set_request)\n",
        "\n",
        "# Step 3: Set up Human Annotation Instructions (ensure it's a valid PDF in GCS)\n",
        "instruction = datalabeling.types.Instruction(\n",
        "    display_name='Animal Classification Instructions',\n",
        "    description='Instructions for labeling images as cats or dogs',\n",
        "    pdf_instruction=datalabeling.types.PdfInstruction(\n",
        "        gcs_file_uri=f'gs://{bucket_name}/labeling-instructions.pdf'  # Ensure the PDF exists in this location\n",
        "    )\n",
        ")\n",
        "\n",
        "instruction_request = datalabeling.CreateInstructionRequest(\n",
        "    parent=project_path,\n",
        "    instruction=instruction\n",
        ")\n",
        "\n",
        "created_instruction = client.create_instruction(request=instruction_request)\n",
        "\n",
        "# Step 4: Define the GcsSource for the dataset\n",
        "gcs_source = datalabeling.types.GcsSource(\n",
        "    input_uri=f'gs://{bucket_name}/images/'  # Ensure the images exist in this location\n",
        ")\n",
        "\n",
        "# Step 5: Define Input Configuration for the dataset\n",
        "input_config = datalabeling.types.InputConfig(\n",
        "    gcs_source=gcs_source\n",
        ")\n",
        "\n",
        "# Step 6: Create the Dataset\n",
        "dataset_request = datalabeling.CreateDatasetRequest(\n",
        "    parent=project_path,\n",
        "    dataset=datalabeling.types.Dataset(\n",
        "        display_name='Animal Image Dataset',\n",
        "        description='Dataset for labeling images of cats and dogs',\n",
        "        input_configs=[input_config]\n",
        "    )\n",
        ")\n",
        "\n",
        "created_dataset = client.create_dataset(request=dataset_request)\n",
        "\n",
        "# Step 7: Submit the Labeling Task\n",
        "labeling_task_request = datalabeling.LabelImageRequest(\n",
        "    parent=project_path,\n",
        "    basic_config=datalabeling.types.HumanAnnotationConfig(\n",
        "        instruction=created_instruction.name,\n",
        "        annotated_dataset_display_name='HITL-Labeling-Job',\n",
        "        language_code='en'\n",
        "    ),\n",
        "    input_configs=[input_config]\n",
        ")\n",
        "\n",
        "client.label_image(request=labeling_task_request)\n",
        "\n",
        "print(\"Labeling job created successfully.\")\n"
      ],
      "metadata": {
        "id": "NNVut54eoE7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Active Learning Implementation:"
      ],
      "metadata": {
        "id": "1BZdV0eTzN3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the Google Cloud SDK if necessary (Colab environment specific)\n",
        "!pip install google-cloud-aiplatform\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Step 3: Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')\n",
        "\n",
        "# Step 4: Define an active learning strategy\n",
        "def active_learning_strategy(predictions, uncertainty_threshold=0.5):\n",
        "    \"\"\"\n",
        "    Identify samples with prediction uncertainty above a threshold.\n",
        "    :param predictions: List of predictions with uncertainty values\n",
        "    :param uncertainty_threshold: The uncertainty threshold for selecting samples\n",
        "    :return: List of indices of uncertain samples\n",
        "    \"\"\"\n",
        "    uncertain_samples = [i for i, p in enumerate(predictions) if p['uncertainty'] > uncertainty_threshold]\n",
        "    return uncertain_samples\n",
        "\n",
        "# Step 5: Load model and dataset\n",
        "model = aiplatform.Model('projects/book-examples-2024/locations/us-central1/models/your-model-id')\n",
        "dataset = aiplatform.ImageDataset('projects/book-examples-2024/locations/us-central1/datasets/your-dataset-id')\n",
        "\n",
        "# Step 6: Predict on unlabeled data\n",
        "# Use batch_predict for generating predictions from unlabeled dataset (Colab execution may need smaller dataset for testing)\n",
        "unlabeled_predictions = model.batch_predict(\n",
        "    gcs_source='gs://my-image-data-bucket-unique/unlabeled-images/',\n",
        "    gcs_destination_prefix='gs://my-image-data-bucket-unique/batch-predictions-output/'\n",
        ")\n",
        "\n",
        "# Step 7: Use active learning strategy to select uncertain samples\n",
        "# Here we simulate predictions. In practice, you'd obtain predictions from the batch_predict output.\n",
        "# This is an example simulation:\n",
        "simulated_predictions = [{'uncertainty': 0.6}, {'uncertainty': 0.3}, {'uncertainty': 0.8}]  # Simulated data\n",
        "uncertain_samples = active_learning_strategy(simulated_predictions)\n",
        "\n",
        "# Step 8: Submit uncertain samples for human labeling\n",
        "labeling_job = aiplatform.LabelingJob.create(\n",
        "    display_name='Active-Learning-Labeling-Job',\n",
        "    dataset=dataset,\n",
        "    labeler_count=5,  # Number of labelers assigned\n",
        "    instruction_uri='gs://my-image-data-bucket-unique/labeling-instructions.pdf',\n",
        "    sample_ids=uncertain_samples  # IDs of uncertain samples to be labeled\n",
        ")\n",
        "\n",
        "print(\"Labeling job submitted for uncertain samples.\")\n"
      ],
      "metadata": {
        "id": "e-cblWo-zSzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Expert Annotation:\n"
      ],
      "metadata": {
        "id": "k2FDz0Qwz0ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install Google Cloud SDK if necessary (Colab specific)\n",
        "!pip install google-cloud-aiplatform\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Step 3: Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')\n",
        "\n",
        "# Step 4: Create a dataset for expert annotation\n",
        "# Ensure your GCS bucket path is correct and images are accessible\n",
        "expert_dataset = aiplatform.ImageDataset.create(\n",
        "    display_name='Expert-Annotated-Dataset',\n",
        "    gcs_source='gs://my-image-data-bucket-unique/path-to-images',  # Update with the correct GCS path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
        ")\n",
        "\n",
        "# Step 5: Assign expert annotators to the labeling task\n",
        "# Provide the correct GCS path for the instruction PDF\n",
        "expert_labeling_job = aiplatform.LabelingJob.create(\n",
        "    display_name='Expert-Labeling-Job',\n",
        "    dataset=expert_dataset,\n",
        "    labeler_count=3,  # Number of expert labelers\n",
        "    instruction_uri='gs://my-image-data-bucket-unique/expert-labeling-instructions.pdf',  # Make sure the instructions PDF is accessible in GCS\n",
        ")\n",
        "\n",
        "print(\"Expert labeling job created successfully.\")\n"
      ],
      "metadata": {
        "id": "2ijQpZN9z28o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Refining Labels with Google Cloud Functions:"
      ],
      "metadata": {
        "id": "XHdZkBlj0VCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the necessary libraries\n",
        "!pip install google-cloud-storage\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import storage\n",
        "\n",
        "# Step 3: Define the function to refine labels based on human feedback\n",
        "def refine_labels(event, context):\n",
        "    \"\"\"\n",
        "    Function to refine labels based on human feedback from a Cloud Storage file.\n",
        "    :param event: Event payload from Cloud Storage (file creation trigger)\n",
        "    :param context: Metadata for the event\n",
        "    \"\"\"\n",
        "    # Initialize the Google Cloud Storage client\n",
        "    client = storage.Client()\n",
        "\n",
        "    # Access the bucket where the human feedback is stored\n",
        "    bucket = client.bucket('my-image-data-bucket-unique')  # Replace with your bucket name\n",
        "\n",
        "    # Access the blob (file) containing human feedback (e.g., JSON format)\n",
        "    blob = bucket.blob('path-to-human-feedback.json')  # Replace with the correct path to your JSON file\n",
        "\n",
        "    # Download the feedback data\n",
        "    feedback_data = blob.download_as_text()\n",
        "\n",
        "    # Process and refine labels (You would define `process_feedback`)\n",
        "    refined_labels = process_feedback(feedback_data)  # Define this function to handle the actual feedback\n",
        "\n",
        "    # Save the refined labels back to Cloud Storage\n",
        "    refined_blob = bucket.blob('path-to-refined-labels.json')  # Path to save refined labels\n",
        "    refined_blob.upload_from_string(refined_labels)\n",
        "\n",
        "    print(\"Labels refined and saved successfully.\")\n",
        "\n",
        "# Step 4: Function to process feedback (Implement based on your specific needs)\n",
        "def process_feedback(feedback_data):\n",
        "    \"\"\"\n",
        "    Example function to process feedback data and return refined labels.\n",
        "    :param feedback_data: The feedback data in JSON or another format\n",
        "    :return: Refined labels as a JSON string\n",
        "    \"\"\"\n",
        "    # Example processing logic (you can define your own)\n",
        "    refined_data = feedback_data  # Placeholder for real logic\n",
        "    return refined_data\n",
        "\n",
        "# Step 5: You can test the function locally in Colab by calling `refine_labels` (with simulated events)\n",
        "# Example of testing the function locally with event and context simulation\n",
        "# Uncomment the next two lines to test the function locally in Colab\n",
        "# event = {'name': 'path-to-human-feedback.json'}  # Simulated event\n",
        "# refine_labels(event, None)\n"
      ],
      "metadata": {
        "id": "2kKynKMb0Wj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example 7.2.3 Augmentation"
      ],
      "metadata": {
        "id": "cuH0DxfR07g-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Data Augmentation Based on Human Feedback:"
      ],
      "metadata": {
        "id": "KTHj2wbd1CQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the necessary libraries (for Colab)\n",
        "!pip install google-cloud-aiplatform\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Step 3: Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')\n",
        "\n",
        "# Step 4: Define a function to generate augmented data based on feedback\n",
        "def augment_data(original_data, feedback):\n",
        "    \"\"\"\n",
        "    Function to augment original data with human feedback variations.\n",
        "    :param original_data: Original dataset\n",
        "    :param feedback: Human feedback (e.g., additional annotations or corrections)\n",
        "    :return: Augmented data\n",
        "    \"\"\"\n",
        "    augmented_data = original_data + feedback_variations(feedback)  # Define feedback_variations based on your use case\n",
        "    return augmented_data\n",
        "\n",
        "# Step 5: Load the original dataset from Google Cloud AI Platform\n",
        "dataset = aiplatform.ImageDataset('projects/book-examples-2024/locations/us-central1/datasets/your-dataset-id')\n",
        "\n",
        "# Step 6: Simulate human feedback (in a real scenario, you'll have feedback from labeling tasks)\n",
        "def get_human_feedback():\n",
        "    \"\"\"Simulate or fetch human feedback\"\"\"\n",
        "    # Simulate feedback\n",
        "    feedback = [{'label': 'cat', 'correction': 'dog'}, {'label': 'dog', 'correction': 'cat'}]\n",
        "    return feedback\n",
        "\n",
        "# Step 7: Generate augmented data using the original dataset and human feedback\n",
        "feedback = get_human_feedback()  # Replace with actual feedback\n",
        "augmented_data = augment_data(dataset, feedback)\n",
        "\n",
        "# Step 8: Save the augmented data to a new dataset in Google Cloud AI Platform\n",
        "augmented_dataset = aiplatform.ImageDataset.create(\n",
        "    display_name='Augmented-Dataset',\n",
        "    gcs_source='gs://my-image-data-bucket-unique/path-to-augmented-images',  # Update with the correct GCS path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
        ")\n",
        "\n",
        "print(\"Augmented dataset created successfully.\")"
      ],
      "metadata": {
        "id": "jplxBh1E0-vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Continuous Improvement and Retraining:"
      ],
      "metadata": {
        "id": "xDUv3prj1WCG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install the necessary libraries (for Colab)\n",
        "!pip install google-cloud-aiplatform\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Step 3: Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')\n",
        "\n",
        "# Step 4: Define the model training function with augmented dataset\n",
        "def retrain_model(augmented_dataset):\n",
        "    \"\"\"\n",
        "    Function to retrain a model with an augmented dataset.\n",
        "    :param augmented_dataset: The augmented dataset created from previous steps\n",
        "    :return: Retrained model\n",
        "    \"\"\"\n",
        "    model = aiplatform.CustomTrainingJob(\n",
        "        display_name='Retrained-Model',\n",
        "        script_path='train_script.py',  # Update this to the path where your training script is stored\n",
        "        model_display_name='Augmented-Model',\n",
        "        container_uri='us-docker.pkg.dev/vertex-ai/training/tf-gpu.2-3:latest'  # TensorFlow container with GPU\n",
        "    )\n",
        "\n",
        "    # Launch training with the augmented dataset\n",
        "    model.run(\n",
        "        dataset=augmented_dataset,  # Using the augmented dataset\n",
        "        model_display_name='Augmented-Model',\n",
        "        training_pipeline_display_name='Augmented-Training-Pipeline',\n",
        "        replica_count=1,\n",
        "        machine_type='n1-standard-4',  # Machine type for training\n",
        "        accelerator_type='NVIDIA_TESLA_T4',  # GPU type\n",
        "        accelerator_count=1  # Number of GPUs\n",
        "    )\n",
        "\n",
        "    print(\"Model retraining job submitted.\")\n",
        "\n",
        "# Step 5: Ensure that the augmented dataset is loaded or created\n",
        "# Assuming that 'augmented_dataset' is already created in a previous step\n",
        "augmented_dataset = aiplatform.ImageDataset(\n",
        "    'projects/book-examples-2024/locations/us-central1/datasets/your-augmented-dataset-id'\n",
        ")\n",
        "\n",
        "# Step 6: Retrain the model with the augmented dataset\n",
        "retrain_model(augmented_dataset)\n"
      ],
      "metadata": {
        "id": "pzCBhMMG1ZEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.3 Leveraging GCP for Human Annotations"
      ],
      "metadata": {
        "id": "PGb_pS1Z1iwY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1: Preparing Your Dataset"
      ],
      "metadata": {
        "id": "OFMtKlfG2Dr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this on the command prompt for uploading a dataset to Google Cloud Storage\n",
        "gsutil cp -r /local-path-to-your-dataset gs://your-bucket/path-to-dataset\n"
      ],
      "metadata": {
        "id": "_Mb6eHT51uOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2: Creating an Annotation Dataset\n",
        "# Step 1: Install the necessary libraries (for Colab)\n",
        "!pip install google-cloud-aiplatform\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Step 3: Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')\n",
        "\n",
        "# Step 4: Create an image dataset for annotation from a GCS source\n",
        "dataset = aiplatform.ImageDataset.create(\n",
        "    display_name='Image-Annotation-Dataset',\n",
        "    gcs_source=['gs://my-image-data-bucket-unique/path-to-dataset'],  # Update with your bucket and dataset path\n",
        "    import_schema_uri=aiplatform.schema.dataset.ioformat.image.single_label_classification,\n",
        ")\n",
        "\n",
        "print(\"Image dataset created successfully.\")\n"
      ],
      "metadata": {
        "id": "5DfR3Uve2Pwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3: Setting Up a Labeling Task\n",
        "# Step 1: Install the necessary libraries (for Colab)\n",
        "!pip install google-cloud-aiplatform\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Step 3: Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')\n",
        "\n",
        "# Step 4: Set up a labeling job for the dataset\n",
        "labeling_job = aiplatform.DataLabelingJob.create(\n",
        "    display_name='Image-Labeling-Job',\n",
        "    dataset=dataset,  # Reference to the dataset created earlier\n",
        "    labeler_count=5,  # Number of human labelers\n",
        "    instruction_uri='gs://my-image-data-bucket-unique/labeling-instructions.pdf',  # Path to the instructions PDF in GCS\n",
        "    annotation_specs=['Cat', 'Dog', 'Bird'],  # Example labels to be used in the annotation\n",
        ")\n",
        "\n",
        "print(\"Labeling job created successfully.\")\n"
      ],
      "metadata": {
        "id": "OyqrU3Da2frY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4: Monitoring the Labeling Process\n",
        "# Step 1: Install the necessary libraries (for Colab)\n",
        "!pip install google-cloud-aiplatform\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "# Step 3: Initialize the AI Platform\n",
        "aiplatform.init(project='book-examples-2024', location='us-central1')\n",
        "\n",
        "# Step 4: Set up a labeling job for the dataset\n",
        "labeling_job = aiplatform.DataLabelingJob.create(\n",
        "    display_name='Image-Labeling-Job',\n",
        "    dataset=dataset,  # Reference to the dataset created earlier\n",
        "    labeler_count=5,  # Number of human labelers\n",
        "    instruction_uri='gs://my-image-data-bucket-unique/labeling-instructions.pdf',  # Path to the instructions PDF in GCS\n",
        "    annotation_specs=['Cat', 'Dog', 'Bird'],  # Example labels to be used in the annotation\n",
        ")\n",
        "\n",
        "# Step 5: Monitor the progress of the labeling job\n",
        "labeling_job.wait()  # This will block until the job is complete\n",
        "\n",
        "# Step 6: Retrieve the status and details of the labeling job\n",
        "status = labeling_job.state\n",
        "details = labeling_job.label_stats\n",
        "\n",
        "print(f\"Labeling Job Status: {status}\")\n",
        "print(f\"Labeling Job Details: {details}\")\n"
      ],
      "metadata": {
        "id": "fq3xpinj2wLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#5: Reviewing and Refining Annotations\n",
        "# Download and review the labeled dataset\n",
        "# This will export the labeled dataset in JSONL format to a GCS bucket\n",
        "labeled_dataset = dataset.export_data(\n",
        "    export_format='jsonl',  # Exporting data in JSON Lines format\n",
        "    gcs_destination='gs://my-image-data-bucket-unique/path-to-labeled-dataset'  # GCS bucket destination for the export\n",
        ")\n",
        "\n",
        "# Wait for the export to complete\n",
        "labeled_dataset.wait()\n",
        "\n",
        "print(\"Labeled dataset export complete. Review the dataset in the GCS bucket.\")\n"
      ],
      "metadata": {
        "id": "XI_3F02G2-wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.4 Training a Custom Reward Model and Preparing Data"
      ],
      "metadata": {
        "id": "A8vHlgZ23t0z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting Data:\n",
        "# Step 1: Install necessary libraries (for Colab)\n",
        "!pip install pandas scikit-learn\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 3: Load your dataset into a pandas DataFrame\n",
        "# Assuming your dataset is already in a pandas DataFrame 'df'\n",
        "# Example:\n",
        "# df = pd.read_csv('/path/to/your/data.csv')\n",
        "\n",
        "# Step 4: Split the dataset into training, validation, and test sets\n",
        "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)  # 20% test set\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)  # 20% validation from remaining 80%\n",
        "\n",
        "# Step 5: Print the sizes of the splits\n",
        "print(f\"Training data size: {len(train_data)}\")\n",
        "print(f\"Validation data size: {len(val_data)}\")\n",
        "print(f\"Test data size: {len(test_data)}\")"
      ],
      "metadata": {
        "id": "jSFUZGzI3vG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Training:\n",
        "# Step 1: Install necessary libraries (for Colab)\n",
        "!pip install pandas scikit-learn\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 3: Load your dataset into a pandas DataFrame\n",
        "# Assuming your dataset is already in a pandas DataFrame 'df'\n",
        "# Example:\n",
        "# df = pd.read_csv('/path/to/your/data.csv')\n",
        "\n",
        "# Step 4: Train a Random Forest model as the reward model\n",
        "# Drop the 'reward' column as it's the target variable\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(train_data.drop(columns=['reward']), train_data['reward'])\n",
        "\n",
        "# Step 5: Validate the model\n",
        "val_predictions = model.predict(val_data.drop(columns=['reward']))\n",
        "validation_score = mean_squared_error(val_data['reward'], val_predictions)\n",
        "\n",
        "# Step 6: Print the validation score (MSE)\n",
        "print(f\"Validation MSE: {validation_score}\")\n"
      ],
      "metadata": {
        "id": "xpWUxgky4CuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deployment and Integration:\n",
        "# Step 1: Install necessary libraries (if not already installed)\n",
        "!pip install joblib\n",
        "\n",
        "# Step 2: Import the joblib library\n",
        "import joblib\n",
        "\n",
        "# Step 3: Save the trained model for later use in the RL framework\n",
        "# 'model' is the trained RandomForestRegressor from the previous steps\n",
        "joblib.dump(model, 'custom_reward_model.pkl')\n",
        "\n",
        "# Step 4: Save the model to Google Drive (optional) or Google Cloud Storage (GCS)\n",
        "# Example for saving to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "joblib.dump(model, '/content/drive/MyDrive/custom_reward_model.pkl')\n",
        "\n",
        "print(\"Model saved successfully.\")\n"
      ],
      "metadata": {
        "id": "CRwH699P4QF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.5 Utilizing Reward Models, including Existing Ones"
      ],
      "metadata": {
        "id": "SqPuEIzx4iNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Integration and Testing:\n",
        "# Step 1: Assuming the RL agent and reward model are already set up\n",
        "# Example of an RL agent integrating the fine-tuned reward model\n",
        "# Pre-trained model is assumed to be loaded from previous steps\n",
        "\n",
        "# Load the pre-trained reward model from a file\n",
        "import joblib\n",
        "pretrained_model = joblib.load('custom_reward_model.pkl')  # Adjust path if necessary\n",
        "\n",
        "# Step 2: Train the RL agent using the reward model\n",
        "# Replace 'rl_agent' with your actual RL agent instance\n",
        "rl_agent.train_with_reward_model(pretrained_model)\n",
        "\n",
        "# Step 3: Monitor the agent's performance to ensure reward signals lead to desired behavior\n",
        "# This would vary depending on your RL framework; the following is an example placeholder\n",
        "performance_metrics = rl_agent.monitor_performance()\n",
        "print(f\"Agent's performance metrics: {performance_metrics}\")\n",
        "\n",
        "# Save or log the agent's performance\n",
        "# Example of logging or storing the performance metrics\n",
        "with open('agent_performance_log.txt', 'w') as f:\n",
        "    f.write(str(performance_metrics))\n",
        "\n",
        "print(\"RL agent's performance monitoring complete.\")\n"
      ],
      "metadata": {
        "id": "c2KEjE-n4jGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.6 Implementing Reinforcement Learning with Proximal Policy Optimization"
      ],
      "metadata": {
        "id": "B6r8C3RU41iA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setting Up the Environment\n",
        "# Step 1: Install necessary libraries (for Colab)\n",
        "!pip install gym\n",
        "\n",
        "# Step 2: Import the gym library\n",
        "import gym\n",
        "\n",
        "# Step 3: Create the CartPole environment\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Step 4: Reset the environment to its initial state\n",
        "state = env.reset()\n",
        "\n",
        "# Step 5: Print the state space and action space\n",
        "print(f\"State space: {env.observation_space}\")\n",
        "print(f\"Action space: {env.action_space}\")\n"
      ],
      "metadata": {
        "id": "wbNI7eDc42dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the PPO Agent\n",
        "# Step 1: Install PyTorch (for Colab)\n",
        "!pip install torch\n",
        "\n",
        "# Step 2: Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 3: Define the policy (actor) network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(128, 128)        # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(128, output_dim) # Output layer\n",
        "        self.softmax = nn.Softmax(dim=-1)     # Softmax activation for probabilities\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))           # ReLU activation for first layer\n",
        "        x = torch.relu(self.fc2(x))           # ReLU activation for second layer\n",
        "        x = self.fc3(x)                       # No activation before softmax\n",
        "        return self.softmax(x)                # Softmax for output\n",
        "\n",
        "# Step 4: Define the value (critic) network\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(128, 128)        # Second fully connected layer\n",
        "        self.fc3 = nn.Linear(128, 1)          # Output layer for value prediction\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))           # ReLU activation for first layer\n",
        "        x = torch.relu(self.fc2(x))           # ReLU activation for second layer\n",
        "        return self.fc3(x)                    # Output value prediction (no activation)\n",
        "\n",
        "# Step 5: Initialize the environment\n",
        "import gym\n",
        "env = gym.make('CartPole-v1')  # Create CartPole environment\n",
        "\n",
        "# Step 6: Initialize the policy and value networks\n",
        "policy_net = PolicyNetwork(input_dim=env.observation_space.shape[0], output_dim=env.action_space.n)\n",
        "value_net = ValueNetwork(input_dim=env.observation_space.shape[0])\n",
        "\n",
        "# Step 7: Define optimizers for the networks\n",
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=3e-4)\n",
        "value_optimizer = optim.Adam(value_net.parameters(), lr=3e-4)\n",
        "\n",
        "# Step 8: Verify initialization\n",
        "print(f\"Policy Network: {policy_net}\")\n",
        "print(f\"Value Network: {value_net}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "TC0_CZ9A5EA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing the PPO Update Function\n",
        "import torch\n",
        "\n",
        "# Define PPO update function\n",
        "def ppo_update(policy_net, value_net, policy_optimizer, value_optimizer, states, actions, rewards, old_log_probs, advantages, clip_param=0.2):\n",
        "    \"\"\"\n",
        "    PPO Update Function\n",
        "\n",
        "    Args:\n",
        "    - policy_net: The policy network (actor).\n",
        "    - value_net: The value network (critic).\n",
        "    - policy_optimizer: Optimizer for the policy network.\n",
        "    - value_optimizer: Optimizer for the value network.\n",
        "    - states: The states from the environment.\n",
        "    - actions: The actions taken by the agent.\n",
        "    - rewards: The observed rewards.\n",
        "    - old_log_probs: The log probabilities from the old policy.\n",
        "    - advantages: The advantage estimates.\n",
        "    - clip_param: The PPO clipping parameter.\n",
        "\n",
        "    Returns:\n",
        "    None (performs in-place updates to the policy and value networks).\n",
        "    \"\"\"\n",
        "    # Calculate the log probabilities of the actions with the current policy\n",
        "    log_probs = torch.log(policy_net(states).gather(1, actions.unsqueeze(1)).squeeze())\n",
        "\n",
        "    # Calculate the ratio of new probabilities to old probabilities\n",
        "    ratios = torch.exp(log_probs - old_log_probs)\n",
        "\n",
        "    # Calculate surrogate objective\n",
        "    surr1 = ratios * advantages\n",
        "    surr2 = torch.clamp(ratios, 1.0 - clip_param, 1.0 + clip_param) * advantages\n",
        "    policy_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "    # Calculate value loss\n",
        "    values = value_net(states).squeeze()\n",
        "    value_loss = (rewards - values).pow(2).mean()\n",
        "\n",
        "    # Update policy network\n",
        "    policy_optimizer.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    policy_optimizer.step()\n",
        "\n",
        "    # Update value network\n",
        "    value_optimizer.zero_grad()\n",
        "    value_loss.backward()\n",
        "    value_optimizer.step()\n",
        "\n",
        "    print(f\"Policy Loss: {policy_loss.item()}, Value Loss: {value_loss.item()}\")\n"
      ],
      "metadata": {
        "id": "hit7iDZe5T3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the PPO Agent\n",
        "import torch\n",
        "\n",
        "# Function to train PPO agent\n",
        "def train_ppo(env, policy_net, value_net, policy_optimizer, value_optimizer, epochs=1000, gamma=0.99):\n",
        "    \"\"\"\n",
        "    Trains the PPO agent using the given environment, policy, and value networks.\n",
        "\n",
        "    Args:\n",
        "    - env: The Gym environment.\n",
        "    - policy_net: The policy network (actor).\n",
        "    - value_net: The value network (critic).\n",
        "    - policy_optimizer: Optimizer for the policy network.\n",
        "    - value_optimizer: Optimizer for the value network.\n",
        "    - epochs: Number of epochs to train for.\n",
        "    - gamma: Discount factor for rewards.\n",
        "\n",
        "    Returns:\n",
        "    None (trains the networks in-place).\n",
        "    \"\"\"\n",
        "    for epoch in range(epochs):\n",
        "        # Reset environment for each epoch\n",
        "        state = env.reset()\n",
        "        rewards, states, actions, log_probs = [], [], [], []\n",
        "\n",
        "        # Rollout episode\n",
        "        for t in range(200):  # Adjust the range for longer/shorter episodes\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "            # Sample action from policy\n",
        "            dist = policy_net(state_tensor)\n",
        "            action = dist.sample()\n",
        "            log_prob = dist.log_prob(action)\n",
        "\n",
        "            # Take action in the environment\n",
        "            next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "            # Store episode information\n",
        "            rewards.append(reward)\n",
        "            states.append(state_tensor)\n",
        "            actions.append(action)\n",
        "            log_probs.append(log_prob)\n",
        "\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Convert lists to tensors\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "\n",
        "        # Calculate returns (discounted rewards)\n",
        "        returns = rewards.flip(dims=(0,)).cumsum(0).flip(dims=(0,)) * gamma\n",
        "\n",
        "        # Calculate advantages: Difference between returns and value predictions\n",
        "        advantages = returns - value_net(torch.cat(states)).detach()\n",
        "\n",
        "        # Perform PPO update\n",
        "        ppo_update(policy_net, value_net, policy_optimizer, value_optimizer,\n",
        "                   torch.cat(states), torch.cat(actions), rewards, torch.cat(log_probs), advantages)\n",
        "\n",
        "        # Print progress every 100 epochs\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}, Total Reward: {sum(rewards)}\")\n",
        "\n",
        "# Initialize the environment (e.g., CartPole)\n",
        "import gym\n",
        "env = gym.make('CartPole-v1')\n",
        "\n",
        "# Assuming 'policy_net' and 'value_net' are already defined\n",
        "# Example: Training the agent with PPO\n",
        "train_ppo(env, policy_net, value_net, policy_optimizer, value_optimizer)\n"
      ],
      "metadata": {
        "id": "hwn_N7Wp5jw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Testing the Trained PPO Agent\n",
        "import torch\n",
        "\n",
        "# Reset the environment to start a new episode\n",
        "state = env.reset()\n",
        "total_reward = 0\n",
        "\n",
        "# Run the environment for 200 steps (or until 'done' signal is received)\n",
        "for _ in range(200):\n",
        "    # Convert the state to a tensor to feed into the policy network\n",
        "    state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    # Get the action from the policy network and sample it\n",
        "    action = policy_net(state_tensor).sample()\n",
        "\n",
        "    # Step in the environment using the sampled action\n",
        "    next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "    # Accumulate rewards\n",
        "    total_reward += reward\n",
        "\n",
        "    # Update the state for the next iteration\n",
        "    state = next_state\n",
        "\n",
        "    # If the episode is done, break the loop\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "# Print the total reward after completing the episode\n",
        "print(f\"Total reward after training: {total_reward}\")\n"
      ],
      "metadata": {
        "id": "NSbPBNfn5yQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.6 Fine-Tuning and Mitigating Reward Hacking Risks"
      ],
      "metadata": {
        "id": "sMhI6tPT6UVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a Shaped Reward Function with PyTorch tensors (if needed) for use in Colab\n",
        "def shaped_reward(state, action, next_state):\n",
        "    \"\"\"\n",
        "    Calculate a shaped reward for the agent's action in the environment.\n",
        "\n",
        "    Args:\n",
        "    - state: The current state of the environment.\n",
        "    - action: The action taken by the agent.\n",
        "    - next_state: The resulting state after the action.\n",
        "\n",
        "    Returns:\n",
        "    - The shaped reward as a combination of base reward, safety bonus, and efficiency bonus.\n",
        "    \"\"\"\n",
        "    # Get the base reward (this could be the reward directly from the environment)\n",
        "    base_reward = get_base_reward(state, action, next_state)\n",
        "\n",
        "    # Calculate safety bonus based on current, action, and next state\n",
        "    safety_bonus = calculate_safety_bonus(state, action, next_state)\n",
        "\n",
        "    # Calculate efficiency bonus based on current, action, and next state\n",
        "    efficiency_bonus = calculate_efficiency_bonus(state, action, next_state)\n",
        "\n",
        "    # Return the total shaped reward\n",
        "    return base_reward + safety_bonus + efficiency_bonus\n",
        "\n",
        "\n",
        "# Sample utility functions (these are placeholders, you should define these)\n",
        "def get_base_reward(state, action, next_state):\n",
        "    \"\"\"Calculate the base reward (e.g., from the environment).\"\"\"\n",
        "    # Placeholder: return a base reward\n",
        "    return 1.0  # Example: flat reward\n",
        "\n",
        "def calculate_safety_bonus(state, action, next_state):\n",
        "    \"\"\"Calculate a bonus for actions that improve safety.\"\"\"\n",
        "    # Placeholder: add logic for safety bonuses based on environment dynamics\n",
        "    return 0.1  # Example: a small safety bonus\n",
        "\n",
        "def calculate_efficiency_bonus(state, action, next_state):\n",
        "    \"\"\"Calculate a bonus for actions that improve efficiency.\"\"\"\n",
        "    # Placeholder: add logic for efficiency bonuses based on environment dynamics\n",
        "    return 0.2  # Example: a small efficiency bonus\n"
      ],
      "metadata": {
        "id": "Q0VzcU1L6VBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Penalizing Undesirable Behaviors:\n",
        "# Example of a Penalizing Reward Function\n",
        "def penalizing_reward(state, action, next_state):\n",
        "    \"\"\"\n",
        "    Calculate a penalizing reward that subtracts points for undesirable actions,\n",
        "    such as collisions or time inefficiency.\n",
        "\n",
        "    Args:\n",
        "    - state: The current state of the environment.\n",
        "    - action: The action taken by the agent.\n",
        "    - next_state: The resulting state after the action.\n",
        "\n",
        "    Returns:\n",
        "    - The penalized reward based on collisions and time penalties.\n",
        "    \"\"\"\n",
        "    # Get the base reward (this could be the reward directly from the environment)\n",
        "    base_reward = get_base_reward(state, action, next_state)\n",
        "\n",
        "    # Apply collision penalty if there's a collision\n",
        "    collision_penalty = -10 if has_collision(state, action, next_state) else 0\n",
        "\n",
        "    # Apply a time penalty based on how long an action takes\n",
        "    time_penalty = -1 * time_taken(state, action, next_state)\n",
        "\n",
        "    # Return the total penalizing reward\n",
        "    return base_reward + collision_penalty + time_penalty\n",
        "\n",
        "\n",
        "# Sample utility functions (you should define the specific logic)\n",
        "def get_base_reward(state, action, next_state):\n",
        "    \"\"\"Calculate the base reward (e.g., from the environment).\"\"\"\n",
        "    # Placeholder: return a base reward\n",
        "    return 1.0  # Example: flat reward\n",
        "\n",
        "def has_collision(state, action, next_state):\n",
        "    \"\"\"Determine if the action led to a collision.\"\"\"\n",
        "    # Placeholder: implement collision logic\n",
        "    return False  # Example: no collision by default\n",
        "\n",
        "def time_taken(state, action, next_state):\n",
        "    \"\"\"Calculate a penalty based on the time taken for an action.\"\"\"\n",
        "    # Placeholder: add logic to penalize time-consuming actions\n",
        "    return 1  # Example: constant time penalty\n"
      ],
      "metadata": {
        "id": "vnjBQa9Y6uLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constrained Optimization:\n",
        "# Example of a Constrained Optimization Reward Function\n",
        "def constrained_reward(state, action, next_state):\n",
        "    \"\"\"\n",
        "    Calculate a reward with a safety constraint. If the safety constraint is violated,\n",
        "    the agent is penalized heavily.\n",
        "\n",
        "    Args:\n",
        "    - state: The current state of the environment.\n",
        "    - action: The action taken by the agent.\n",
        "    - next_state: The resulting state after the action.\n",
        "\n",
        "    Returns:\n",
        "    - The constrained reward, which includes penalties for violating safety constraints.\n",
        "    \"\"\"\n",
        "    # Get the base reward (this could be the reward directly from the environment)\n",
        "    base_reward = get_base_reward(state, action, next_state)\n",
        "\n",
        "    # Apply a heavy penalty if safety constraints are violated\n",
        "    safety_constraint_penalty = -100 if violates_safety_constraint(state, action, next_state) else 0\n",
        "\n",
        "    # Return the total reward with the constraint penalty applied\n",
        "    return base_reward + safety_constraint_penalty\n",
        "\n",
        "\n",
        "# Sample utility functions (you should define the specific logic)\n",
        "def get_base_reward(state, action, next_state):\n",
        "    \"\"\"Calculate the base reward (e.g., from the environment).\"\"\"\n",
        "    # Placeholder: return a base reward\n",
        "    return 1.0  # Example: flat reward\n",
        "\n",
        "def violates_safety_constraint(state, action, next_state):\n",
        "    \"\"\"Check if the safety constraint is violated.\"\"\"\n",
        "    # Placeholder: add logic to detect safety violations\n",
        "    return False  # Example: no violation by default\n",
        "\n"
      ],
      "metadata": {
        "id": "P233oybY68W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.7 Incorporating PEFT in Reinforcement Learning"
      ],
      "metadata": {
        "id": "xLNItYJI7JZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Identifying Tunable Parameters\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example of selecting tunable parameters in the policy network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        \"\"\"\n",
        "        Initializes the PolicyNetwork with tunable and non-tunable layers.\n",
        "\n",
        "        Args:\n",
        "        - input_dim: Number of input features (dimensions).\n",
        "        - output_dim: Number of output actions (dimensions).\n",
        "        \"\"\"\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)  # Non-tunable layer\n",
        "        self.fc2 = nn.Linear(128, 128)        # Non-tunable layer\n",
        "        self.fc3 = nn.Linear(128, output_dim)  # Tunable layer\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "        # Freeze the parameters of the first two layers (fc1 and fc2)\n",
        "        for param in self.fc1.parameters():\n",
        "            param.requires_grad = False\n",
        "        for param in self.fc2.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for the policy network.\n",
        "\n",
        "        Args:\n",
        "        - x: Input state.\n",
        "\n",
        "        Returns:\n",
        "        - Output action probabilities.\n",
        "        \"\"\"\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "input_dim = 4  # Example input dimensions (e.g., from an environment state)\n",
        "output_dim = 2  # Example output dimensions (e.g., action space)\n",
        "\n",
        "# Initialize the policy network\n",
        "policy_net = PolicyNetwork(input_dim, output_dim)\n",
        "\n",
        "# Print out the tunable and frozen parameters\n",
        "for name, param in policy_net.named_parameters():\n",
        "    print(f\"{name} - requires_grad: {param.requires_grad}\")\n"
      ],
      "metadata": {
        "id": "g-umj8787KKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing Fine-Tuning with PEFT Techniques:\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import loralib as lora  # Make sure you have LoRA library installed\n",
        "\n",
        "# Define a Policy Network\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return self.softmax(x)\n",
        "\n",
        "# Initialize the Policy Network\n",
        "input_dim = 4  # Example input dimension (e.g., state space)\n",
        "output_dim = 2  # Example output dimension (e.g., action space)\n",
        "policy_network = PolicyNetwork(input_dim, output_dim)\n",
        "\n",
        "# Apply LoRA to the selected layer (fc3)\n",
        "lora.apply_lora(policy_network.fc3, rank=8)\n",
        "\n",
        "# Define Adapter for additional transformation\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self, input_dim, adapter_dim):\n",
        "        \"\"\"\n",
        "        Adapter layers provide an additional transformation.\n",
        "        Args:\n",
        "        - input_dim: Input dimension of the adapter (matches the layer size).\n",
        "        - adapter_dim: The internal dimension for down-projection.\n",
        "        \"\"\"\n",
        "        super(Adapter, self).__init__()\n",
        "        self.adapter = nn.Sequential(\n",
        "            nn.Linear(input_dim, adapter_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(adapter_dim, input_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.adapter(x)  # Skip connection\n",
        "\n",
        "# Apply the adapter to the selected layer (fc3)\n",
        "policy_network.fc3 = nn.Sequential(policy_network.fc3, Adapter(128, 32))\n",
        "\n",
        "# Example forward pass\n",
        "example_state = torch.rand((1, input_dim))  # Example input state\n",
        "output = policy_network(example_state)\n",
        "print(output)\n"
      ],
      "metadata": {
        "id": "tEzRCaH_7aXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine-Tuning the Model:\n",
        "import torch\n",
        "\n",
        "# Assuming policy_network is already initialized and modified with LoRA and Adapter layers\n",
        "# LoRA applied to policy_network.fc3 in earlier code\n",
        "# Adapter applied to policy_network.fc3 in earlier code\n",
        "\n",
        "# Define optimizer for the fc3 layer (with LoRA and Adapter layers applied)\n",
        "optimizer = torch.optim.Adam(policy_network.fc3.parameters(), lr=1e-4)\n",
        "\n",
        "# Define loss function (e.g., cross-entropy for classification tasks)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Number of epochs and dataloader assumed to be predefined\n",
        "num_epochs = 10  # Example number of epochs\n",
        "dataloader = ...  # Define your own dataloader for your task\n",
        "\n",
        "# Fine-tuning loop\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass through the policy network\n",
        "        states = batch['state']  # Input states from batch\n",
        "        actions = batch['action']  # True actions from batch\n",
        "\n",
        "        outputs = policy_network(states)  # Forward pass with modified policy network\n",
        "\n",
        "        # Calculate loss between outputs and true actions\n",
        "        loss = criterion(outputs, actions)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the network parameters\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YGhXeteO7lri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.8 Evaluating Model Performance Qualitatively and Quantitatively"
      ],
      "metadata": {
        "id": "AEsFivIW71h4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.8.1 Quantitative Evaluation"
      ],
      "metadata": {
        "id": "PhqlOFTR75K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy, Precision, Recall, and F1 Score:\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# True labels and predicted labels (binary classification example)\n",
        "y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]  # Ground truth (actual labels)\n",
        "y_pred = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]  # Model predictions\n",
        "\n",
        "# Calculate Precision, Recall, and F1 score\n",
        "precision = precision_score(y_true, y_pred)  # Precision calculation\n",
        "recall = recall_score(y_true, y_pred)  # Recall calculation\n",
        "f1 = f1_score(y_true, y_pred)  # F1 Score calculation\n",
        "\n",
        "# Display results\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "id": "Ci-RewuF72WA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Confusion Matrix:\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# True labels and predicted labels (same as before)\n",
        "y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix as a heatmap using Seaborn\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "\n",
        "# Add labels to the plot\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "282LRYIF8J2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC-AUC Score:\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# True labels and predicted labels (binary classification example)\n",
        "y_true = [0, 1, 1, 1, 0, 1, 0, 0, 1, 0]\n",
        "y_pred = [0, 1, 0, 1, 0, 1, 0, 1, 1, 0]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_true, y_pred)\n",
        "\n",
        "# Print the ROC-AUC Score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.2f}\")"
      ],
      "metadata": {
        "id": "xkYVdpEa8YUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean Squared Error (MSE) and R-squared:\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# True labels and predicted labels (example for regression)\n",
        "y_true = [3.0, -0.5, 2.0, 7.0]\n",
        "y_pred = [2.5, 0.0, 2.1, 7.8]\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "\n",
        "# Calculate R-squared (R)\n",
        "r_squared = r2_score(y_true, y_pred)\n",
        "\n",
        "# Print the results\n",
        "print(f\"Mean Squared Error: {mse:.2f}\")\n",
        "print(f\"R-squared: {r_squared:.2f}\")\n"
      ],
      "metadata": {
        "id": "-bclg4_L8k1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7.9 Loading Evaluation Model and Aggregating Evaluation Metrics for Comparison"
      ],
      "metadata": {
        "id": "-Co76XXt8zjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the Evaluation Model:\n",
        "import joblib\n",
        "\n",
        "# Load the saved model from a pickle file\n",
        "model = joblib.load('path_to_model.pkl')\n",
        "\n",
        "# Example: Prepare the evaluation dataset (Replace with actual function or code)\n",
        "def load_test_data():\n",
        "    # Dummy test data as an example\n",
        "    X_test = [[1.5, 2.0], [3.0, 4.5], [5.2, 6.1]]  # Features\n",
        "    y_test = [0, 1, 1]  # Labels\n",
        "    return X_test, y_test\n",
        "\n",
        "# Load the test data\n",
        "X_test, y_test = load_test_data()\n",
        "\n",
        "# Now you can use `model` for prediction or further evaluation\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print predictions and compare to actual test labels\n",
        "print(f\"Predictions: {y_pred}\")\n",
        "print(f\"True Labels: {y_test}\")\n"
      ],
      "metadata": {
        "id": "sZVO2Rx180Un"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluating the Model:\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# Generate predictions using the pre-trained model on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "\n",
        "# Store the metrics in a dictionary\n",
        "metrics = {\n",
        "    'accuracy': accuracy,\n",
        "    'precision': precision,\n",
        "    'recall': recall\n",
        "}\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n"
      ],
      "metadata": {
        "id": "9Jl-V8Fm9Bmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Aggregating Evaluation Metrics:\n",
        "import pandas as pd\n",
        "\n",
        "# Example metrics for two models\n",
        "metrics_1 = {'model': 'Model 1', 'accuracy': 0.95, 'precision': 0.93, 'recall': 0.92}\n",
        "metrics_2 = {'model': 'Model 2', 'accuracy': 0.94, 'precision': 0.92, 'recall': 0.91}\n",
        "\n",
        "# Aggregate the metrics into a DataFrame for comparison\n",
        "df_metrics = pd.DataFrame([metrics_1, metrics_2])\n",
        "\n",
        "# Display the aggregated metrics comparison\n",
        "print(df_metrics)\n"
      ],
      "metadata": {
        "id": "1IG5fetR9NVk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}